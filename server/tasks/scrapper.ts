import type { Category } from '~~/types/category'
import type { DomElement, Element } from 'domhandler'
import type { NewChunk } from '../utils/drizzle'
import { createHash } from 'node:crypto'
import { mkdir, writeFile } from 'node:fs/promises'
import consola from 'consola'
import select from 'css-select'
import { parseDocument } from 'htmlparser2'
import { $fetch } from 'ofetch'

import TurndownService from 'turndown'

const debugSlugs: string[] = [
  // '10-consejos-para-no-procrastinar',
  // 'causas-de-la-procrastinacion',
  // 'como-ser-mas-productivo',
  // 'aprender-a-organizarse',
  // 'como-crear-un-habito',
  // 'disciplina-es-igual-a-libertad',
]

const folder = 'content/blog'

const categories: Category[] = []

type BlogPostProperties = Pick<NewPostRecord, 'modifiedAt' | 'publishedAt' | 'scrappedAt'> & {
  title: string
  subtitle: string
  url: string
  slug: string
  categories: string[]
  image?: string
  imageUrl: string
  audioLink?: string
  bibliography: string[]
}

export type BlogPostPropertiesStringified = {
  [K in keyof BlogPostProperties]: string | undefined
}

const getSlug = (link: string) => link.split('/').at(-2)!

export default defineTask({
  meta: {
    name: 'scraper',
    description: 'Run the scraper',
  },
  async run() {
    if (!import.meta.dev)
      throw createError('This task is only available in development mode')

    consola.log('Running Scraper')

    await mkdir(folder, { recursive: true })
    let counter = 0
    await handleCategories(categories)

    for await (const postLink of getPosts()) {
      const slug = getSlug(postLink)
      const postRecord = await useDrizzle().select().from(tables.postRecord).where(eq(tables.postRecord.slug, slug)).get()
      const alreadyStoredTimestamps = postRecord ? { publishedAt: postRecord?.publishedAt, modifiedAt: postRecord?.modifiedAt } : undefined
      const post = await parsePost(postLink, { alreadyStoredTimestamps })
      if (!post) {
        consola.warn(`We already have ${slug} up-to-date. Skipping...`)
        continue
      }
      const { content, chunks } = post

      const embedding = await hubAI().run('@cf/baai/bge-base-en-v1.5', { text: chunks.map(chunk => chunk.content) })
      const vectors = embedding.data

      const newPostRecord: NewPostRecord = { modifiedAt: post.frontmatter.modifiedAt, publishedAt: post.frontmatter.publishedAt, scrappedAt: post.frontmatter.scrappedAt, slug }

      let postRecordId = -1

      if (postRecord)
        postRecordId = await useDrizzle().update(tables.postRecord).set(newPostRecord).where(eq(tables.postRecord.slug, slug)).returning({ id: tables.postRecord.id }).then(res => res.at(0)!.id)
      else
        postRecordId = await useDrizzle().insert(tables.postRecord).values(newPostRecord).returning({ id: tables.postRecord.id }).then(res => res[0].id)
      if (postRecordId === -1)
        throw new Error(`error updating post record ${postRecordId}: ${slug}`)

      await Promise.all(chunks.map((chunk, i) => {
        const newChunk = { ...chunk, embedding: vectors[i], postRecordId }
        return useDrizzle().insert(tables.chunks).values(newChunk).returning({ insertedId: tables.chunks.id })
      }))

      consola.info(`Writing ./content/blog/${slug}.md`)
      const filePath = `${folder}/${slug}.md`
      await writeFile(filePath, content)
      counter++
    }

    consola.info(`Scraped ${counter} posts`)
    return { result: 'Success' }
  },
})

async function handleCategories(categories: Category[]) {
  const categoryColors = {
    'estilo-de-vida': '#16a34a',
    'desarrollo-personal': '#6366f1',
    'masculinidad': '#ea580c',
    'fitness': '#22c55e',
    'nutricion': '#f59e0b',
    'habitos-saludables': '#4ade80',
    'productividad': '#2563eb',
    'inversion': '#d97706',
    'analisis-tecnico': '#0e7490',
    'criptomonedas': '#f97316',
    'impuestos': '#ca8a04',
    'dinero': '#15803d',
    'relaciones': '#be185d',
    'seduccion': '#dc2626',
    'estilo': '#a855f7',
    'mitos-nutricionales': '#f43f5e',
    'relaciones-de-pareja': '#db2777',
    'bitcoin': '#f97316',
    'habilidades-sociales': '#0ea5e9',
    'mundo': '#0f766e',
    'antiveganismo': '#991b1b',
    'radiaciones-electromagn': '#9333ea',
    'negocios': '#047857',
    'enigmas-de-la-historia': '#7e22ce',
    'ejercicios-empuje-horizontal': '#f87171',
    'ejercicios-empuje-vertical': '#fb923c',
    'minimalismo': '#6b7280',
  }

  const removeEmoji = (str: string) => str.replace(/[\p{Emoji}\u{FE0F}\u{FE0E}]/gu, '').trim()

  const structuredCategories = categories.map(category => ({
    slug: category.slug,
    label: removeEmoji(category.label),
    color: categoryColors[category.slug as keyof typeof categoryColors] || '#000000',
  }))

  await writeFile('app/composables/categories.ts', `
    // This file is generated by the scraper task
    export const categories = ${JSON.stringify(structuredCategories, null, 2)}
  `.trim())
}

const removeLinesContaining = [
  '/\\*! elementor - ',
  'data-mce-type="bookmark"',
]

interface Post {
  frontmatter: BlogPostProperties
  content: string
  chunks: NewChunk[]
}

interface ParsePostOptions {
  alreadyStoredTimestamps?: Pick<BlogPostProperties, 'publishedAt' | 'modifiedAt'>
}

export async function parsePost(link: string, options: ParsePostOptions = {}): Promise<Post | undefined> {
  // Fetch and parse the HTML
  consola.info('Fetching', link)
  const html = await $fetch(link, { parseResponse: txt => txt })
  const dom = parseDocument(html) as DomElement

  const _modifiedAt = (select('meta[property="article:modified_time"]', dom)[0] as DomElement)?.attribs?.content || ''
  const _publishedAt = (select('meta[property="article:published_time"]', dom)[0] as DomElement)?.attribs?.content || ''
  const modifiedAt = new Date(_modifiedAt)
  const isModifiedValid = modifiedAt instanceof Date && !Number.isNaN(modifiedAt.getTime())
  const publishedAt = new Date(_publishedAt)
  const hasModifiedAt = !Number.isNaN(modifiedAt.getTime())

  // First check if we have valid published dates to compare
  const samePublishedAt = options.alreadyStoredTimestamps?.publishedAt.getTime() === publishedAt.getTime()

  if (hasModifiedAt) {
    // If we have a new modified date, compare with stored one if it exists
    const sameModifiedAt = options.alreadyStoredTimestamps?.modifiedAt?.getTime() === modifiedAt.getTime()
    if (samePublishedAt && sameModifiedAt)
      return
  }
  else {
    // If no modified date, only check published date
    if (samePublishedAt) {
      return
    }
  }

  const turndownService = new TurndownService()
  const { audioLink, bibliography, md } = cleanUpMarkdown(turndownService.turndown(html))

  // Frontmatter
  const slug = getSlug(link)
  const title = (select('h1', dom)[0]?.children?.[0] as Element)?.data.trim().replace(/"/g, '\\"').trim() || ''
  const subtitle = (select('h1 .subtitulo', dom)[0]?.children?.[0] as Element)?.data.trim().replace(/"/g, '\\"').trim() || ''

  const imageUrl = (select('meta[property="og:image"]', dom)[0] as DomElement)?.attribs?.content || ''
  const image = imageUrl.split('/').at(-1)
  const url = `https://pau.ninja/${slug}`
  const categoryRe = /(?:Mira mis otros artÃ­culos sobre|O si quieres ser mÃ¡s especÃ­fico):\s*((?:\[.*?\]\(https:\/\/pau\.ninja\/.*?\/\),?\s*)+)/g
  const categoryMatches = Array.from(md.matchAll(categoryRe))
  const categories: string[] = categoryMatches.map(match => match[1])
  const scrappedAt = new Date()

  const frontmatter: BlogPostProperties = { title, subtitle, url, slug, categories, image, imageUrl, publishedAt, modifiedAt, scrappedAt, audioLink, bibliography }
  const categoriesStr = categories.join(', ')
  const bibliographyStr = bibliography.join('\n')
  const frontmatterObject: BlogPostPropertiesStringified = {
    ...frontmatter,
    categories: categoriesStr,
    bibliography: bibliographyStr,
    scrappedAt: scrappedAt.toISOString(),
    publishedAt: publishedAt.toISOString(),
    modifiedAt: isModifiedValid ? modifiedAt.toISOString() : undefined,
  }
  const frontmatterStr = Object.entries(frontmatterObject)
    .map(([key, value]) => `${key}: ${value}`)
    .join('\n')
  const content = `---\n${frontmatterStr}\n---\n\n${md}`

  const chunks = getMarkdownChunks(md, frontmatter)

  return { content, frontmatter, chunks }
}

function cleanUpMarkdown(md: string): { md: string, bibliography: string[], audioLink: string } {
  const lines = md.split('\n')
  const dateLineIndex = lines.findIndex(line => line.includes('ðŸ“… Actualizado'))
  if (dateLineIndex === -1)
    throw new Error('Could not find the line with the date')

  // Remove lines up to the date line
  md = lines
    .slice(dateLineIndex + 1)
    .filter(line => !removeLinesContaining.some(phrase => line.includes(phrase)))
    .join('\n')
    .trim()

  // Extract bibliography entries
  const bibliographyRe = /\*\s+\d+\s+([^\n]+)/g
  const bibliographyMatches = md.match(bibliographyRe) || []
  const bibliography = bibliographyMatches.map(entry => entry.replace(/\*\s+\d+\s+/, ''))
  for (const entry of bibliography) {
    md = md.replace(entry, '')
  }

  // Extract audio link
  const audioRe = /^\[.*\]\((https?:\/\/[^)]+)\)/g
  const audioLinkMatch = md.match(audioRe) || []
  const audioLink = audioLinkMatch.map(match => match.replace(audioRe, '$1'))[0]

  const crapAtTheEnd = /\n.*: fuentes, referencias y notas[\s\S]*/
  const crapAtTheEnd2 = /\nFuentes, referencias y notas[\s\S]*/
  const crapAtTheEnd3 = /\.autoPodcast ul li a img\{width:50px\}(.*)/s
  const toc = /Navega por el contenido\n\n\[Toggle\]\(#\)\n\n(\*.*\n(\s*\*.*\n)*)/g
  const h2Markup = /(.*)\n-+/g

  md = md
    .replace(toc, '')
    .replace(crapAtTheEnd, '')
    .replace(crapAtTheEnd2, '')
    .replace(crapAtTheEnd3, '')
    .replace(h2Markup, '## $1')
    .replace(audioRe, '')
    .replace(/\n{3,}/g, '\n\n')
    .replace(/\[(\d+)\]\(javascript:void\\\(0\\\)\)/g, '[ref-$1](#ref-$1){.ref}')
    .replace(/â–²/g, '')
    .replace(/â–¼/g, '')
    .replace(/https:\/\/pau\.ninja\/(.*)\//g, './$1')
    .replace(/^\*\s+/gm, '- ')
    .trim()

  return { md, bibliography, audioLink }
}
function getMarkdownChunks(content: string, { title }: BlogPostProperties): NewChunk[] {
  const lines = content.split('\n')
  const chunks: NewChunk[] = []
  let currentSection: NewChunk | undefined
  let currentContent: string[] = []
  let currentHeadings: string[] = [title]

  for (const line of lines) {
    if (line.startsWith('#')) {
      // Save previous section if exists
      if (currentSection) {
        currentSection.content = currentContent.join('\n').trim()
        if (currentSection.content.length > 25) {
          const hash = createHash('md5').update(currentSection.content).digest('hex')
          chunks.push({ ...currentSection, hash })
        }
        currentContent = []
      }

      // Start new section
      const level = line.match(/^#+/)![0].length
      const heading = line.slice(level).trim()

      // Update headings array based on level
      currentHeadings = currentHeadings.slice(0, level - 1)
      currentHeadings[level - 1] = heading

      currentSection = { headers: currentHeadings.slice(0, level) as [string, string, string, string], content: '', hash: '' }
    }
    else if (currentSection) {
      currentContent.push(line)
    }
    else if (title) {
      // Create initial section for content before first heading
      currentSection = { headers: [title], hash: '', content: '' }
      if (line.trim())
        currentContent.push(line)
    }
  }

  // Add final section
  if (currentSection) {
    currentSection.content = currentContent.join('\n').trim()
    if (currentSection.content.length > 25) {
      const hash = createHash('md5').update(currentSection.content).digest('hex')
      chunks.push({ ...currentSection, hash })
    }
  }

  return chunks
}

async function* getPosts(): AsyncGenerator<string, void, void> {
  const url = 'https://pau.ninja'

  if (debugSlugs.length > 0) {
    consola.info(`Scrapping just ${debugSlugs}`)
    for (const slug of debugSlugs) {
      yield `${url}/${slug}/`
    }
    return
  }

  let pageNumber = 1

  while (true) {
    const pageUrl = `${url}/blog/${pageNumber++}`
    const html = await $fetch(pageUrl, { responseType: 'text' })
    const dom = parseDocument(html)
    const links = select('.elementor-widget-container > div > article > a', dom) as DomElement[]

    if (links.length === 0)
      break

    for (const link of links) {
      yield link.attribs!.href
    }
  }
}
