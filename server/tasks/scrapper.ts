import type { Category } from '~~/types/category'
import type { DomElement, Element } from 'domhandler'
import { mkdir, writeFile } from 'node:fs/promises'
import consola from 'consola'
import select from 'css-select'
import { parseDocument } from 'htmlparser2'
import TurndownService from 'turndown'
import { folder } from '../utils/content'

const categories: Category[] = []

export default defineTask({
  meta: {
    name: 'scraper',
    description: 'Run the scraper',
  },
  async run() {
    if (!import.meta.dev)
      throw createError('This task is only available in development mode')

    consola.log('Running Scraper')

    await mkdir(folder, { recursive: true })
    let counter = 0

    for await (const post of getPost()) {
      consola.log(`Writing ./content/blog/${post.slug}.md`)
      const filePath = `${folder}/${post.slug}.md`
      await writeFile(filePath, post.markdown)
      counter++
    }
    await handleCategories(categories)
    consola.info(`Scraped ${counter} posts with the following categories: ${categories.map(c => c.label).join(', ')}`)
    return { result: 'Success' }
  },
})

async function handleCategories(categories: Category[]) {
  const categoryColors = {
    'estilo-de-vida': '#16a34a',
    'desarrollo-personal': '#6366f1',
    'masculinidad': '#ea580c',
    'fitness': '#22c55e',
    'nutricion': '#f59e0b',
    'habitos-saludables': '#4ade80',
    'productividad': '#2563eb',
    'inversion': '#d97706',
    'analisis-tecnico': '#0e7490',
    'criptomonedas': '#f97316',
    'impuestos': '#ca8a04',
    'dinero': '#15803d',
    'relaciones': '#be185d',
    'seduccion': '#dc2626',
    'estilo': '#a855f7',
    'mitos-nutricionales': '#f43f5e',
    'relaciones-de-pareja': '#db2777',
    'bitcoin': '#f97316',
    'habilidades-sociales': '#0ea5e9',
    'mundo': '#0f766e',
    'antiveganismo': '#991b1b',
    'radiaciones-electromagn': '#9333ea',
    'negocios': '#047857',
    'enigmas-de-la-historia': '#7e22ce',
    'ejercicios-empuje-horizontal': '#f87171',
    'ejercicios-empuje-vertical': '#fb923c',
    'minimalismo': '#6b7280',
  }

  const removeEmoji = (str: string) => str.replace(/[\p{Emoji}\u{FE0F}\u{FE0E}]/gu, '').trim()

  const structuredCategories = categories.map(category => ({
    slug: category.slug,
    label: removeEmoji(category.label),
    color: categoryColors[category.slug as keyof typeof categoryColors] || '#000000',
  }))

  await writeFile('app/composables/categories.ts', `
// This file is generated by the scraper task
export const categories = ${JSON.stringify(structuredCategories, null, 2)}
`.trim())
}

const removeLinesContaining = [
  '/\\*! elementor - ',
  'data-mce-type="bookmark"',
]

async function parsePost(link: string) {
  const res = await fetch(link)
  const html = await res.text()
  const dom = parseDocument(html)

  const turndownService = new TurndownService()
  let md = turndownService.turndown(html)
  const lines = md.split('\n')
  const crap = lines.findIndex(line => line.includes('ðŸ“… Actualizado'))
  if (crap === -1)
    throw new Error('Could not find the line with the date')

  md = lines
    .slice(crap + 1)
    .filter(line => !removeLinesContaining.some(phrase => line.includes(phrase)))
    .join('\n')

  const bibliographyRe = /\*\s+\d+\s+([^\n]+)/g
  const match = md.match(bibliographyRe)

  const bibliographyEntries = match?.map(entry => entry.replace(/\*\s+\d+\s+/, '')) || []
  for (const entry of bibliographyEntries) {
    md = md.replaceAll(entry, '')
  }

  const audioRe = /^\[.*\]\((https?:\/\/[^)]+)\)/g
  const audioLink = (md.trimStart().match(audioRe) || []).map(match => match.replace(audioRe, '$1'))

  const slug = link.split('/').at(-2)
  const name = (select('h1', dom)[0]?.children[0] as Element)?.data.trim() || ''
  const imageUrl = (select('meta[property="og:image"]', dom)[0] as DomElement)?.attribs?.content || ''
  const imageFilename = imageUrl.split('/').at(-1)
  const published = (select('meta[property="article:published_time"]', dom)[0] as DomElement)?.attribs?.content || ''
  const modified = (select('meta[property="article:modified_time"]', dom)[0] as DomElement)?.attribs?.content || ''
  const url = `https://pau.ninja/${slug}`

  const categoryRe = /(?:Mira mis otros artÃ­culos sobre|O si quieres ser mÃ¡s especÃ­fico):\s*((?:\[.*?\]\(https:\/\/pau\.ninja\/.*?\/\),?\s*)+)/g
  const categoryMatches = Array.from(md.matchAll(categoryRe))

  const postCategories: Category[] = []

  categoryMatches.forEach((match) => {
    const individualCategoryRe = /\[(.*?)\]\(https:\/\/pau\.ninja\/(.*?)\/\)/g
    const individualMatches = Array.from(match[1].matchAll(individualCategoryRe))

    individualMatches.forEach((indMatch) => {
      const category: Category = { slug: indMatch[2], label: indMatch[1] }
      if (!postCategories.some(c => c.slug === category.slug)) {
        postCategories.push(category)
      }
    })
  })

  // Update categories array
  postCategories.forEach((category) => {
    if (!categories.some(c => c.slug === category.slug)) {
      categories.push(category)
    }
  })

  // remove everything from bliography to the end
  const crapAtTheEnd = /\nBibliografÃ­a: fuentes, referencias y notas[\s\S]*/

  // remove the table of contents
  const toc = /Navega por el contenido\n\n\[Toggle\]\(#\)\n\n(\*.*\n(\s*\*.*\n)*)/g

  const h2Markup = /(.*)\n-+/g

  // clean up the markdown
  md = md
    .replace(audioRe, '')
    .replace(toc, '')
    .replace(crapAtTheEnd, '')
    .replace(h2Markup, '## $1')
    .replaceAll(/\n{3,}/g, '\n\n')
    .replaceAll(/\[(\d+)\]\(javascript:void\\\(0\\\)\)/g, '[ref-$1](#ref-$1){.ref}')
    .replaceAll(/â–²/g, '')
    .replaceAll(/â–¼/g, '')
    .replaceAll(/https:\/\/pau\.ninja\/(.*)\//g, './$1')
    .replaceAll(/^\*\s+/gm, '- ')
    .trim()

  const frontmatter = {
    title: `"${name.replace(/"/g, '\\"').trim()}"`,
    url,
    slug,
    categories: JSON.stringify(postCategories.map(c => c.slug)),
    image: imageFilename,
    imageURL: imageUrl,
    published,
    modified,
    scrappedAt: new Date().toISOString(),
    audioLink,
    bibliography: JSON.stringify(bibliographyEntries),
  }
  const frontmatterStr = Object.entries(frontmatter).map(([key, value]) => `${key}: ${value}`).join('\n')

  const markdown = `---\n${frontmatterStr}\n---\n${md}`

  return {
    slug,
    markdown,
  }
}

/**
 * Loop over https://pau.ninja/blog/{N}
 *
 * The return value is HTML, so we need to parse it, and select
 * the links we want to keep with css-select's select() method.
 *
 * The moment we don't have any more links, we stop the loop. We yield the links we found one-by-one.
 */
async function* getPost(): AsyncGenerator<any, void, void> {
  const url = 'https://pau.ninja/blog'

  let i = 1

  while (true) {
    const res = await fetch(`${url}/${i++}`)
    const html = await res.text()
    const dom = parseDocument(html)
    const links = select('.elementor-widget-container > div > article > a', dom) as DomElement[]
    if (links.length === 0)
      break
    for (const link of links) {
      yield parsePost(link.attribs!.href)
    }
  }
}
